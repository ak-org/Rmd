---
title: "HW6.Rmd"
author : "Ashish Kumar // ashishk2@illinois.edu"
output: html_document
---
# 1. Linear regression with various regularizers 
```{r message=FALSE}
rm(list = ls())
library(stats)
library(ggplot2)
library(gdata)
library(caret)
library(caTools)
library(glmnet)
library(plotmo)
library(geosphere)
library(MASS)
library(ModelMetrics)
library(knitr)
data = read.table("~/cs498aml/music/default_plus_chromatic_features_1059_tracks.txt", header = FALSE, sep = ",")

```
## First, build a straightforward linear regression of latitude (resp. longitude) against features. What is the R-squared? 

```{r}
set.seed(2)
reg1.lm = lm(V117 ~ . - V118, data=data)
reg2.lm = lm(V118 ~ . - V117, data=data)
rsq_table = data.frame(Items=c("No transformation "), Lat = c(summary(reg1.lm)$r.squared), Long = c(summary(reg2.lm)$r.squared))
```
## Plot a graph evaluating each regression.
```{r}
plot(reg1.lm, which=1:6)
```

```{r}
plot(reg2.lm, which=1:6)
```

## Convert lat/long to positive values

```{r}
data = read.table("~/cs498aml/music/default_plus_chromatic_features_1059_tracks.txt", header = FALSE, sep = ",")
data$V117 = data$V117 + 90
data$V118 = data$V118 + 180

reg3.lm = lm(V117 ~ . - V118, data=data)
reg4.lm = lm(V118 ~ . - V117, data=data)

rsq_table = rbind(rsq_table, data.frame(Items=c("Positive transformation "), Lat = c(summary(reg3.lm)$r.squared), Long = c(summary(reg4.lm)$r.squared)))

```


## Does a Box-Cox transformation improve the regressions?
> Box Cox tranformation improved the regression as evident by the improved R-Squared value below.

```{r}
bxc = MASS::boxcox(reg3.lm, lambda = seq(-5, 15, 1/10), plotit = TRUE)
lambda = bxc$x[which(bxc$y == max(bxc$y))]
data$V117 = (data$V117^lambda - 1)/lambda
regbx.lm = lm(V117 ~ . -V118, data=data)
print(paste("Box Cox Lambda ", lambda))
plotres(regbx.lm, info=TRUE, caption="Box Cox Tranformation", which = 1:9)
```
```{r}
bxc = MASS::boxcox(reg4.lm, lambda = seq(-5, 15, 1/10), plotit = TRUE)
lambda = bxc$x[which(bxc$y == max(bxc$y))]
data$V118 = (data$V118^lambda - 1)/lambda
regbx2.lm = lm(V118 ~ . -V117, data=data)
print(paste("Box Cox Lambda ", lambda))
rsq_table = rbind(rsq_table, data.frame(Items=c("Box Cox transformation "), Lat = c(summary(regbx.lm)$r.squared), Long = c(summary(regbx2.lm)$r.squared)))
plotres(regbx2.lm, info=TRUE, caption="Box Cox Tranformation", which = 1:9)
```

## R-Squared values

```{r kable1, results = 'asis', warning=FALSE, message=FALSE}
knitr::kable(rsq_table, format="markdown", digits=6, align=c('l','c','c'), padding=10, caption="R-Squared Table")
```

## Use glmnet to produce a unregularized regression (equivalently, a value of lambda = 0)

```{r}
set.seed(2)
noreg1.model = cv.glmnet(as.matrix(data[,-c(117,118)]), data$V117, family="gaussian", lambda = seq(0,0.000001, 0.000001), alpha=0)
noreg2.model = cv.glmnet(as.matrix(data[,-c(117,118)]), data$V118, family="gaussian", lambda = seq(0,0.000001, 0.000001), alpha=0)
cvllat_table = data.frame(Items=c("Unregularized "), 
                       CVErr = c(noreg1.model$cvm[2]), 
                       Lambda = c(noreg1.model$lambda[2]))
cvllong_table = data.frame(Items=c("Unregularized "),
                       CVErr = c(noreg2.model$cvm[2]), 
                       Lambda = c(noreg2.model$lambda[2]))

```


## Use glmnet to produce a regression regularized by L2 (equivalently, a ridge regression)

```{r}
set.seed(2)
ridge1.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V117,  family="gaussian", nlambda = 300, alpha=0)
idx1 = which(ridge1.model$cvm == min(ridge1.model$cvm))
ridge2.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V118,  family="gaussian", nlambda = 300, alpha=0)
idx2 = which(ridge2.model$cvm == min(ridge2.model$cvm))
cvllat_table = rbind(cvllat_table,
            data.frame(Items=c("Ridge"), 
                       CVErr = c(min(ridge1.model$cvm)), 
                       Lambda = c(ridge1.model$lambda[idx1])))
cvllong_table = rbind(cvllong_table,  
             data.frame(Items=c("Ridge"),                      
                       CVErr = c(min(ridge2.model$cvm)), 
                       Lambda = c(ridge2.model$lambda[idx2])))

```

```{r}
plotres(ridge1.model, info=TRUE, caption = "Ridge Model (Lat.)", which = 1:9)
```
```{r}
plotres(ridge2.model, info=TRUE, caption = "Ridge Model (Long.)", which = 1:9)
```

###  Is the regularized regression better than the unregularized regression?

> Regularized regression is better. The CV Error is less as compared to non-regularized regression.

## Use glmnet to produce a regression regularized by L1 (equivalently, a lasso regression)

```{r}
set.seed(2)
lasso1.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V117, family="gaussian", nlambda = 300, alpha=1.0)
lasso2.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V118, family="gaussian", nlambda = 300, alpha=1.0)
```
```{r}
plotres(lasso1.model, info=TRUE, caption = "Lasso Model (Lat.)", which = 1:9)
plotres(lasso2.model, info=TRUE, caption = "Lasso Model (Long.)", which = 1:9)
```
```{r}
idx1 = which(lasso1.model$cvm == min(lasso1.model$cvm))
idx2 = which(lasso2.model$cvm == min(lasso2.model$cvm))
cvllat_table = rbind(cvllat_table,
            data.frame(Items=c("Lasso"), 
                       CVErr = c(min(lasso1.model$cvm)), 
                       Lambda = c(lasso1.model$lambda[idx1])))
cvllong_table = rbind(cvllong_table,  
             data.frame(Items=c("Lasso"), 
                       CVErr = c(min(lasso2.model$cvm)), 
                       Lambda = c(lasso2.model$lambda[idx2])))
```

### How many variables are used by this regression? 

```{r}
print(paste( "Parameters used (Lat.): ",  lasso1.model$nzero[idx1], ", Parameters used (Long.): ", lasso2.model$nzero[idx2]))
```

### Is the regularized regression better than the unregularized regression?
```{r kable2, results = 'asis', warning=FALSE, message=FALSE}
knitr::kable(cvllat_table, format="markdown", 
             digits=6, align=c('l','c','c'), padding=10, caption="Latitude Error Comparison")
knitr::kable(cvllong_table, format="markdown", 
             digits=6, align=c('l','c','c'), padding=10, caption="Longitude Error Comparison")
```

> The CV error for regularized regression is smaller than the unregularized regression.


## Use glmnet to produce a regression regularized by elastic net (equivalently, a regression regularized by a convex combination of L1 and L2). Try three values of alpha, the weight setting how big L1 and L2 are. 

 

```{r}
set.seed(2)
elnetlat_tbl = data.frame()
elnetlong_tbl = data.frame()
ii = 1
for (l in c(seq(0.1,0.9,0.1))) {
  elnet1.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V117, family="gaussian", nlambda = 300, alpha=l)
  idx1 = which(elnet1.model$cvm == min(elnet1.model$cvm))
  elnetlat_tbl = rbind(elnetlat_tbl,
                    data.frame(Idx = c(ii), Alpha = c(l),
                               CVM = c(elnet1.model$cvm[idx1]),
                               Params = c(elnet1.model$nzero[idx1]),
                               Lambda = c(elnet1.model$lambda[idx1])))
 ii = ii + 1
}
ii = 1
for (l in c(seq(0.1,0.9,0.1))) {
  elnet2.model = cv.glmnet(as.matrix(as.matrix(data[,-c(117,118)])), data$V118, family="gaussian", nlambda = 300, alpha=l)
  idx2 = which(elnet2.model$cvm == min(elnet2.model$cvm))
  elnetlong_tbl = rbind(elnetlong_tbl,
                    data.frame(Idx = c(ii), Alpha = c(l),
                               CVM = c(elnet2.model$cvm[idx2]),
                               Params = c(elnet2.model$nzero[idx2]),
                               Lambda = c(elnet2.model$lambda[idx2]))) 
  ii = ii + 1
}

```

### How many variables are used by this regression?Is the regularized regression better than the unregularized regression?

### Latitude Parameter
```{r kable3, results = 'markup', warning=FALSE, message=FALSE}
knitr::kable(elnetlat_tbl, format="markdown", 
             digits=6, align=c('c','c','c','c','c'), padding=10, caption="Alpha Comparison")
```
### Longitude Parameter
```{r kable4, results = 'markup', warning=FALSE, message=FALSE}
knitr::kable(elnetlong_tbl, format="markdown", 
             digits=6, align=c('c','c','c','c','c'), padding=10, caption="Alpha Comparison")
```


> Number or variables used vary by the value of alpha. The range of variable is listed for lat. and long. in the table above.
> In general, The CV error varies with alpha values. The regularizer does not consistently improves the CV error for the regression model.

# 2. Logistic regression 

```{r message=FALSE}
options(scipen = 0)
rm(list = ls())
library(stats)
library(ggplot2)
library(gdata)
library(caret)
library(caTools)
library(glmnet)
library(plotmo)
set.seed(2018)
```
```{r echo = FALSE}

#X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. 
#X2: Gender (1 = male; 2 = female). 
#X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). 
#X4: Marital status (1 = married; 2 = single; 3 = others). 
#X5: Age (year). 
#X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the #repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The #measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = #payment delay for eight months; 9 = payment delay for nine months and above. 
#X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, #2005; . . .; X17 = amount of bill statement in April, 2005. 
#X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount #paid in April, 2005. 
```
```{r}
webLink = "http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
localLink = "~/cs498aml/default of credit card clients.xls"
data2 = read.xls(localLink, header = TRUE, sheet = 1)
data = data2[,-c(1)] #index column adds no value, ditch it

```

### run simple glm to detect outliers

```{r}
glm.model = glm(Y ~ ., data=data, family="binomial")
plotres(glm.model, which = 1, npoints = -1)

```

### Partition data into test and train

```{r}
idx = createDataPartition(y=data$Y, p = 0.8, list=FALSE)
trainData = data[idx,]
testData = data[-idx,]

```

### Simple logistic Regression 

```{r}
elnet.model = glmnet(as.matrix(trainData[,-c(24)]), as.factor(trainData$Y), family="binomial", nlambda = 300, alpha=0.5)
lasso.model = glmnet(as.matrix(trainData[,-c(24)]), as.factor(trainData$Y), family="binomial", nlambda = 300, alpha=1.0)
ridge.model = glmnet(as.matrix(trainData[,-c(24)]), as.factor(trainData$Y), family="binomial", nlambda = 300, alpha=0)

plot_glmnet(lasso.model, xvar="lambda", label=25)
plot_glmnet(ridge.model, xvar="lambda", label=25)
plot_glmnet(elnet.model, xvar="lambda", label=25)


```

```{r}
for (l in 0:10) {
  assign(paste("model.fit", l, sep=""), cv.glmnet(as.matrix(trainData[,-c(24)]), as.factor(trainData$Y), family="binomial", alpha=l/10))
}
```

### Lasso model 

```{r}
plotres(model.fit10, main="LASSO", info=TRUE, which=1:9)
```

### Elastic Net model 

```{r}
plotres(model.fit5, main="Elastic Net",info=TRUE, which=1:9)
```

### Ridge model 

```{r}
plotres(model.fit0, main="Ridge",info=TRUE, which=1:9)

```

### Simple logistic Regression (alpha 0 to 1 in 0.1 increments)

```{r}
yhat0 = predict(model.fit0, s = model.fit0$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat1 = predict(model.fit1, s = model.fit1$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat2 = predict(model.fit2, s = model.fit2$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat3 = predict(model.fit3, s = model.fit3$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat4 = predict(model.fit4, s = model.fit4$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat5 = predict(model.fit5, s = model.fit5$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat6 = predict(model.fit6, s = model.fit6$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat7 = predict(model.fit7, s = model.fit7$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat8 = predict(model.fit8, s = model.fit8$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat9 = predict(model.fit9, s = model.fit9$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat10 = predict(model.fit10, s = model.fit10$lambda.1se, type="class", newx = as.matrix(testData[,-24]))

acc= c(1:11)
acc[1] = mean(as.factor(yhat0) == testData$Y)
acc[2] = mean(as.factor(yhat1) == testData$Y)
acc[3] = mean(as.factor(yhat2) == testData$Y)
acc[4] = mean(as.factor(yhat3) == testData$Y)
acc[5] = mean(as.factor(yhat4) == testData$Y)
acc[6] = mean(as.factor(yhat5) == testData$Y)
acc[7] = mean(as.factor(yhat6) == testData$Y)
acc[8] = mean(as.factor(yhat7) == testData$Y)
acc[9] = mean(as.factor(yhat8) == testData$Y)
acc[10] = mean(as.factor(yhat9) == testData$Y)
acc[11] = mean(as.factor(yhat10) == testData$Y)
plot(seq(0,1,0.1), acc, type="o", xlab="Alpha", ylab="Accuracy", sub=paste("Max Accuracy (1se Lambda)=",max(acc)))
```

### Using Lambda min value
```{r}
yhat0 = predict(model.fit0, s = model.fit0$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat1 = predict(model.fit1, s = model.fit1$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat2 = predict(model.fit2, s = model.fit2$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat3 = predict(model.fit3, s = model.fit3$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat4 = predict(model.fit4, s = model.fit4$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat5 = predict(model.fit5, s = model.fit5$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat6 = predict(model.fit6, s = model.fit6$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat7 = predict(model.fit7, s = model.fit7$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat8 = predict(model.fit8, s = model.fit8$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat9 = predict(model.fit9, s = model.fit9$lambda.min, type="class", newx = as.matrix(testData[,-24]))
yhat10 = predict(model.fit10, s = model.fit10$lambda.min, type="class", newx = as.matrix(testData[,-24]))

acc= c(1:11)
acc[1] = mean(as.factor(yhat0) == testData$Y)
acc[2] = mean(as.factor(yhat1) == testData$Y)
acc[3] = mean(as.factor(yhat2) == testData$Y)
acc[4] = mean(as.factor(yhat3) == testData$Y)
acc[5] = mean(as.factor(yhat4) == testData$Y)
acc[6] = mean(as.factor(yhat5) == testData$Y)
acc[7] = mean(as.factor(yhat6) == testData$Y)
acc[8] = mean(as.factor(yhat7) == testData$Y)
acc[9] = mean(as.factor(yhat8) == testData$Y)
acc[10] = mean(as.factor(yhat9) == testData$Y)
acc[11] = mean(as.factor(yhat10) == testData$Y)
plot(seq(0,1,0.1), acc, type="o", xlab="Alpha", ylab="Accuracy", sub=paste("Max Accuracy (Min Lambda)=",max(acc)))
```

```{r}
```
### logistic regression after removing outliers

```{r}
data = data2[-c(5925,20893),-c(1)] #ditch outliers
set.seed(1)
idx = createDataPartition(y=data$Y, p = 0.8, list=FALSE)
trainData = data[idx,]
testData = data[-idx,]
for (l in 0:10) {
  assign(paste("model.fit", l, sep=""), cv.glmnet(as.matrix(trainData[,-c(24)]), as.factor(trainData$Y), family="binomial", alpha=l/10))
}
yhat0 = predict(model.fit0, s = model.fit0$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat1 = predict(model.fit1, s = model.fit1$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat2 = predict(model.fit2, s = model.fit2$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat3 = predict(model.fit3, s = model.fit3$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat4 = predict(model.fit4, s = model.fit4$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat5 = predict(model.fit5, s = model.fit5$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat6 = predict(model.fit6, s = model.fit6$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat7 = predict(model.fit7, s = model.fit7$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat8 = predict(model.fit8, s = model.fit8$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat9 = predict(model.fit9, s = model.fit9$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
yhat10 = predict(model.fit10, s = model.fit10$lambda.1se, type="class", newx = as.matrix(testData[,-24]))
acc= c(1:11)
acc[1] = mean(as.factor(yhat0) == testData$Y)
acc[2] = mean(as.factor(yhat1) == testData$Y)
acc[3] = mean(as.factor(yhat2) == testData$Y)
acc[4] = mean(as.factor(yhat3) == testData$Y)
acc[5] = mean(as.factor(yhat4) == testData$Y)
acc[6] = mean(as.factor(yhat5) == testData$Y)
acc[7] = mean(as.factor(yhat6) == testData$Y)
acc[8] = mean(as.factor(yhat7) == testData$Y)
acc[9] = mean(as.factor(yhat8) == testData$Y)
acc[10] = mean(as.factor(yhat9) == testData$Y)
acc[11] = mean(as.factor(yhat10) == testData$Y)
plot(seq(0,1,0.1), acc, type="o", xlab="Alpha", ylab="Accuracy", sub=paste("Max Accuracy (Remove outlier) =",max(acc)))

```


## Conclusion

> The best accuray of ~ 81% is obtained when we keep the outliers and alpha is 0.9. When we remove couple of outliers the accuracy declines slightly.

